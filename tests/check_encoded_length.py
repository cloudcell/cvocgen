#!/usr/bin/env python3
"""
Compare the encoding lengths of molecules using vocabularies generated by
APETokenizer and cvocgen.

This script:
1. Generates vocabularies with 30 merges using both APETokenizer and cvocgen
2. Encodes the test data using each vocabulary
3. Compares the min/max/average encoded lengths for each
"""

import os
import json
import subprocess
import argparse
import statistics
import sys
from collections import defaultdict
import matplotlib.pyplot as plt

# Add the parent directory to the path so we can import APETokenizer
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) 
from apetokenizer.ape_tokenizer import APETokenizer


def load_corpus(file_path):
    """Load corpus from a file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines()]


def generate_ape_vocabulary(input_file, output_dir, num_merges=30):
    """Generate vocabulary using APETokenizer with specified number of merges."""
    print(f"Generating APETokenizer vocabulary with {num_merges} merges...")
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Create tokenizer instance
    tokenizer = APETokenizer()
    
    # Load corpus
    corpus = load_corpus(input_file)
    
    # Train the tokenizer
    tokenizer.train(
        corpus=corpus,
        type="selfies",  # Assuming SELFIES format
        max_vocab_size=5000,
        min_freq_for_merge=2000,
        max_merges=num_merges
    )
    
    # Save the vocabulary
    vocab_path = os.path.join(output_dir, "ape_vocab.json")
    tokenizer.save_vocabulary(vocab_path)
    
    print(f"APETokenizer vocabulary size: {len(tokenizer.vocabulary)}")
    
    return tokenizer


def generate_cvocgen_vocabulary(input_file, output_dir, num_merges=30):
    """Generate vocabulary using cvocgen with specified number of merges."""
    print(f"\nGenerating cvocgen vocabulary with {num_merges} merges...")
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Run cvocgen command with output directory
    cmd = [
        "./bin/cvocgen",
        "-f", input_file,
        "-n", str(num_merges),
        "-t", "selfies",
        "-o", output_dir
    ]
    
    try:
        # Run cvocgen with stdout directly to terminal for real-time output
        result = subprocess.run(
            cmd,
            text=True,
            check=True
        )
        print("cvocgen execution completed successfully")
    except subprocess.CalledProcessError as e:
        print(f"Error running cvocgen: {e}")
        print(f"stdout: {e.stdout if hasattr(e, 'stdout') else ''}")
        print(f"stderr: {e.stderr if hasattr(e, 'stderr') else ''}")
        return None
    
    # Look for the vocabulary file
    vocab_file = os.path.join(output_dir, f"vocab_{num_merges}.json")
    if not os.path.exists(vocab_file):
        print(f"Error: cvocgen did not create vocabulary file {vocab_file}")
        return None
    
    # Load the vocabulary
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # Check if the JSON is missing the closing brace
            if content.strip().endswith(','):
                content = content.strip()[:-1] + '}'
            elif not content.strip().endswith('}') and not content.strip().endswith(']'): 
                content = content.strip() + '}'
                
            try:
                vocab_data = json.loads(content)
                print(f"Successfully loaded vocabulary from {vocab_file}")
                print(f"cvocgen vocabulary size: {len(vocab_data)}")
                
                # Handle different JSON formats
                if isinstance(vocab_data, dict):
                    # If it's already a dictionary, use it directly
                    print("Loaded vocabulary as frequency dictionary")
                elif isinstance(vocab_data, list):
                    # If it's a list of tokens, convert to a dictionary with default frequency of 1
                    print("Loaded vocabulary as token list, converting to frequency dictionary")
                    vocab_freq = {}
                    for token in vocab_data:
                        vocab_freq[token] = 1
                    vocab_data = vocab_freq
                    
                return vocab_data
                    
            except json.JSONDecodeError as e:
                print(f"Error: Could not parse JSON in {vocab_file}: {e}")
                # Try a more aggressive approach - parse line by line
                print("Attempting to parse JSON line by line...")
                vocab_freq = {}
                
                # Reset file pointer
                with open(vocab_file, 'r', encoding='utf-8') as f2:
                    lines = f2.readlines()
                
                # Skip first line (opening brace) and parse each key-value pair
                for line in lines[1:]:  # Skip the first line which should be '{' or '['
                    line = line.strip()
                    if not line or line == '{' or line == '}' or line == '[' or line == ']': 
                        continue
                        
                    # Remove trailing comma if present
                    if line.endswith(','):
                        line = line[:-1]
                        
                    try:
                        # Try to parse the line as a JSON fragment
                        key_value = '{' + line + '}'
                        pair = json.loads(key_value)
                        # Add the key-value pair to our dictionary
                        for k, v in pair.items():
                            vocab_freq[k] = v
                    except json.JSONDecodeError:
                        # If it's not a key-value pair, it might be a token in a list
                        try:
                            # Try to parse as a string token
                            token = json.loads(line)
                            if isinstance(token, str):
                                vocab_freq[token] = 1
                        except:
                            # Skip lines that can't be parsed
                            continue
                
                if vocab_freq:
                    print(f"Successfully parsed {len(vocab_freq)} tokens from {vocab_file} using line-by-line method")
                    return vocab_freq
    except Exception as e:
        print(f"Error loading vocabulary file {vocab_file}: {e}")
        return None


def encode_with_ape(tokenizer, corpus):
    """Encode the corpus using APETokenizer."""
    print("\nEncoding corpus with APETokenizer...")
    encoded_lengths = []
    
    for i, molecule in enumerate(corpus):
        # Use the tokenizer to encode the molecule
        encoded = tokenizer.encode(molecule, add_special_tokens=False)
        encoded_lengths.append(len(encoded))
        
        # Print progress every 10000 molecules
        if (i + 1) % 10000 == 0:
            print(f"Processed {i + 1}/{len(corpus)} molecules")
    
    return encoded_lengths


def encode_with_cvocgen(vocab_data, corpus):
    """Encode the corpus using cvocgen vocabulary but with APETokenizer's encode function."""
    print("\nEncoding corpus with cvocgen vocabulary (using APETokenizer.encode)...")
    
    # Create a new APETokenizer instance
    tokenizer = APETokenizer()
    
    # Replace its vocabulary with the cvocgen vocabulary
    # First, preserve special tokens
    special_tokens = tokenizer.special_tokens.copy()
    
    # Clear existing vocabulary
    tokenizer.vocabulary = dict(special_tokens)
    
    # Add cvocgen tokens to the vocabulary
    next_id = len(special_tokens)
    if isinstance(vocab_data, dict):
        # Sort by frequency (highest first) to match BPE merge priority
        sorted_tokens = sorted(vocab_data.items(), key=lambda x: x[1], reverse=True)
        for token, _ in sorted_tokens:
            if token not in tokenizer.vocabulary:  # Skip if it's a special token
                tokenizer.vocabulary[token] = next_id
                next_id += 1
    else:
        # If it's a list, add tokens in order
        for token in vocab_data:
            if token not in tokenizer.vocabulary:  # Skip if it's a special token
                tokenizer.vocabulary[token] = next_id
                next_id += 1
    
    # Update the reverse vocabulary
    tokenizer.update_reverse_vocabulary()
    
    print(f"Loaded cvocgen vocabulary into APETokenizer (size: {len(tokenizer.vocabulary)})")
    
    # Now encode using this tokenizer
    encoded_lengths = []
    for i, molecule in enumerate(corpus):
        # Use the tokenizer to encode the molecule
        encoded = tokenizer.encode(molecule, add_special_tokens=False)
        encoded_lengths.append(len(encoded))
        
        # Print progress every 10000 molecules
        if (i + 1) % 10000 == 0:
            print(f"Processed {i + 1}/{len(corpus)} molecules")
    
    return encoded_lengths


def compare_encoding_lengths(ape_lengths, cvoc_lengths):
    """Compare the encoding lengths between APETokenizer and cvocgen."""
    print("\n=== Encoding Length Comparison ===\n")
    
    # Calculate statistics
    ape_min = min(ape_lengths)
    ape_max = max(ape_lengths)
    ape_avg = statistics.mean(ape_lengths)
    ape_median = statistics.median(ape_lengths)
    
    cvoc_min = min(cvoc_lengths)
    cvoc_max = max(cvoc_lengths)
    cvoc_avg = statistics.mean(cvoc_lengths)
    cvoc_median = statistics.median(cvoc_lengths)
    
    # Print comparison
    print(f"{'Metric':<15} {'APETokenizer':<15} {'cvocgen':<15} {'Difference':<15} {'Ratio (APE/cvocgen)':<20}")
    print("-" * 70)
    print(f"{'Min Length':<15} {ape_min:<15} {cvoc_min:<15} {ape_min - cvoc_min:<15} {ape_min / cvoc_min if cvoc_min > 0 else 'N/A':<20.4f}")
    print(f"{'Max Length':<15} {ape_max:<15} {cvoc_max:<15} {ape_max - cvoc_max:<15} {ape_max / cvoc_max if cvoc_max > 0 else 'N/A':<20.4f}")
    print(f"{'Avg Length':<15} {ape_avg:<15.2f} {cvoc_avg:<15.2f} {ape_avg - cvoc_avg:<15.2f} {ape_avg / cvoc_avg if cvoc_avg > 0 else 'N/A':<20.4f}")
    print(f"{'Median Length':<15} {ape_median:<15} {cvoc_median:<15} {ape_median - cvoc_median:<15} {ape_median / cvoc_median if cvoc_median > 0 else 'N/A':<20.4f}")
    
    # Create a histogram
    plt.figure(figsize=(12, 6))
    plt.hist(ape_lengths, bins=50, alpha=0.5, label='APETokenizer')
    plt.hist(cvoc_lengths, bins=50, alpha=0.5, label='cvocgen')
    plt.xlabel('Encoded Length')
    plt.ylabel('Frequency')
    plt.title('Distribution of Encoded Lengths')
    plt.legend()
    
    # Save the histogram
    hist_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'test_results', 'encoding_length_histogram.png')
    os.makedirs(os.path.dirname(hist_path), exist_ok=True)
    plt.savefig(hist_path)
    print(f"\nHistogram saved to {hist_path}")
    
    return {
        'ape_min': ape_min,
        'ape_max': ape_max,
        'ape_avg': ape_avg,
        'ape_median': ape_median,
        'cvoc_min': cvoc_min,
        'cvoc_max': cvoc_max,
        'cvoc_avg': cvoc_avg,
        'cvoc_median': cvoc_median,
    }


def main():
    parser = argparse.ArgumentParser(description="Compare encoding lengths between APETokenizer and cvocgen")
    parser.add_argument("--output-dir", type=str, default="./test_results", 
                        help="Directory to store output files")
    parser.add_argument("--input-file", type=str, default="./data/test.selfies.unique.txt", 
                        help="Input corpus file")
    parser.add_argument("--merges", type=int, default=30, 
                        help="Number of merges to perform")
    parser.add_argument("--sample-size", type=int, default=0,
                        help="Number of molecules to sample (0 for all)")
    args = parser.parse_args()
    
    # Use the command-line parameters
    output_dir = args.output_dir
    input_file = args.input_file
    num_merges = args.merges
    sample_size = args.sample_size
    
    print(f"Using output directory: {output_dir}")
    print(f"Input file: {input_file}")
    print(f"Number of merges: {num_merges}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Check if the input file exists, if not, try to unpack from gz file
    if not os.path.exists(input_file):
        print(f"Input file {input_file} not found, checking for gzipped version...")
        gz_file = f"{input_file}.gz"
        if os.path.exists(gz_file):
            print(f"Found gzipped file {gz_file}, unpacking...")
            try:
                import gzip
                import shutil
                # Extract the directory path from input_file
                data_dir = os.path.dirname(input_file)
                # Ensure the directory exists
                os.makedirs(data_dir, exist_ok=True)
                
                # Unpack the gz file
                with gzip.open(gz_file, 'rb') as f_in:
                    with open(input_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                print(f"Successfully unpacked {gz_file} to {input_file}")
            except Exception as e:
                print(f"Error unpacking gzipped file: {e}")
                import traceback
                traceback.print_exc()
                return
        else:
            print(f"Error: Neither {input_file} nor {gz_file} found")
            return
    
    # Load the corpus
    corpus = load_corpus(input_file)
    print(f"Loaded corpus with {len(corpus)} molecules")
    
    # Sample the corpus if requested
    if sample_size > 0 and sample_size < len(corpus):
        import random
        random.seed(42)  # For reproducibility
        corpus = random.sample(corpus, sample_size)
        print(f"Sampled {sample_size} molecules from the corpus")
    
    try:
        # Generate vocabularies
        ape_tokenizer = generate_ape_vocabulary(input_file, output_dir, num_merges)
        cvoc_vocab = generate_cvocgen_vocabulary(input_file, output_dir, num_merges)
        
        if ape_tokenizer is None or cvoc_vocab is None:
            print("Error: Failed to generate vocabularies")
            return
        
        # Encode the corpus
        ape_lengths = encode_with_ape(ape_tokenizer, corpus)
        cvoc_lengths = encode_with_cvocgen(cvoc_vocab, corpus)
        
        # Compare encoding lengths
        stats = compare_encoding_lengths(ape_lengths, cvoc_lengths)
        
        # Save the results
        results_file = os.path.join(output_dir, "encoding_length_comparison.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'statistics': stats,
                'ape_lengths': ape_lengths,
                'cvoc_lengths': cvoc_lengths
            }, f, indent=4)
        print(f"\nResults saved to {results_file}")
        
        # Calculate and print total lengths
        ape_total = sum(ape_lengths)
        cvoc_total = sum(cvoc_lengths)
        print("\n=== Total Encoding Length Comparison ===\n")
        print(f"APETokenizer total encoded length: {ape_total:,}")
        print(f"cvocgen total encoded length:      {cvoc_total:,}")
        print(f"Difference (APE - cvocgen):        {ape_total - cvoc_total:,}")
        print(f"Ratio (APE/cvocgen):               {ape_total/cvoc_total:.4f}")
        
    except Exception as e:
        print(f"Error during encoding comparison: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
